{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sub-activity: Loading and pre-processing of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "    \n",
    "    wikidata_ID_params = {\n",
    "    \"action\":\"query\",\n",
    "    \"format\":\"json\",\n",
    "    \"list\":\"search\",\n",
    "    \"srprop\":\"sectiontitle\",\n",
    "    \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"srlimit\":100\n",
    "    }\n",
    "\n",
    "    wikidata_ID_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_ID_params)\n",
    "    wikidata_ID_data = wikidata_ID_response.json()    \n",
    "    wikidata_IDs = [entity_ID[\"title\"] for entity_ID in wikidata_ID_data[\"query\"][\"search\"]]\n",
    "\n",
    "    return wikidata_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_IDs = get_turing_award_recipients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(wikidata_ID):\n",
    "    wikipedia_API_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    WIKIDATA_GET_CONTENT_PARAMS = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "    \n",
    "    wikidata_response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_GET_CONTENT_PARAMS)\n",
    "    wikidata_response_data = wikidata_response.json()\n",
    "\n",
    "    # To extract content from the wikipidia page, we have to use titles gained the wikidata IDs, since the titles of wikipedia pages are unique.\n",
    "    wikidata_title = wikidata_response_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "\n",
    "    wikipedia_content_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": wikidata_title,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"exsectionformat\": \"wiki\",\n",
    "    }\n",
    "\n",
    "    wikipedia_content_response = requests.get(wikipedia_API_endpoint, params = wikipedia_content_params)\n",
    "    wikipedia_content_data = wikipedia_content_response.json()\n",
    "    wikipedia_content = wikipedia_content_data[\"query\"][\"pages\"][0][\"extract\"]\n",
    "    return wikipedia_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_winners_info(wikidata_ID):\n",
    "    \n",
    "    wikidata_params = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"props\": \"claims|sitelinks\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"languages\": \"en\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "\n",
    "    wikidata_title_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params)\n",
    "    wikidata_title_data = wikidata_title_response.json()\n",
    "\n",
    "    # Extract name\n",
    "    try:\n",
    "        wikidata_name = wikidata_title_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "    except KeyError:\n",
    "        wikidata_name = None\n",
    "\n",
    "    # Extract intro from wikipedia page\n",
    "    try:\n",
    "        wikipedia_content = get_wikipedia_content(wikidata_ID)\n",
    "    except KeyError:\n",
    "        wikipedia_intro = None\n",
    "    else:\n",
    "        content_with_p_tag = re.sub(r\"<\\/?(?!p)\\w*\\b[^>]*>\", \"\", wikipedia_content.split(\"<h2>\")[0])\n",
    "        content_remove_backslash = re.sub(r\"\\\\\",\"\", content_with_p_tag)\n",
    "        content_remove_newline_to_space = re.sub(r\"\\n\",\" \", content_remove_backslash)\n",
    "        paragraphs = re.findall(r'<p>(.+?)</p>', content_remove_newline_to_space)\n",
    "        wikipedia_intro = \" \\n\".join(paragraphs)\n",
    "\n",
    "    # Extract gender ID to get gender from \"sex or gender (P21)\"\n",
    "    try:\n",
    "        wikidata_gender_id = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P21\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_gender_id = None\n",
    "        \n",
    "    # Get birth date from \"date of birth (P569)\"\n",
    "    try:\n",
    "        wikidata_birth_date = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P569\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"].split(\"T\")[0].split(\"+\")[1]\n",
    "    except KeyError:\n",
    "        wikidata_birth_date = None\n",
    "\n",
    "    # Extract birth place ID to get birth place from \"place of birth (P19)\"\n",
    "    try:\n",
    "        wikidata_birth_place = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P19\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_birth_place = None\n",
    "\n",
    "    # Extract employer ID to get employer from \"employer (P108)\"\n",
    "    # employer ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_employer_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P108\"]\n",
    "    except KeyError:\n",
    "        wikidata_employers_IDs = None\n",
    "    else:\n",
    "        wikidata_employers_IDs = [wikidata_employer_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_employer_ID in wikidata_employer_mainsnaks]\n",
    "\n",
    "    # Extract educated at ID to get educated at from \"educated at (P69)\"\n",
    "    # educated at ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_educated_at_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P69\"]\n",
    "    except KeyError:\n",
    "        wikidata_educated_at_IDs = None\n",
    "    else:\n",
    "        wikidata_educated_at_IDs = [wikidata_educated_at_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_educated_at_ID in wikidata_educated_at_mainsnaks]\n",
    "\n",
    "    # Join the IDs per person that we want to get info from into a list\n",
    "    entity_info_request_IDs = [wikidata_gender_id, wikidata_birth_place, \"|\".join(wikidata_employers_IDs), \"|\".join(wikidata_educated_at_IDs)]\n",
    "    \n",
    "    # A list that contains all the info of the person\n",
    "    entity_info = [wikidata_name, wikipedia_intro, wikidata_birth_date]\n",
    "    for i in range(len(entity_info_request_IDs)):\n",
    "        entity_values = []\n",
    "\n",
    "        wikidata_params_2 = {\n",
    "            \"action\":\"wbgetentities\",\n",
    "            \"format\":\"json\",\n",
    "            \"ids\": entity_info_request_IDs[i],\n",
    "            \"props\": \"labels\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"languages\": \"en\",\n",
    "            \"sitefilter\": \"enwiki\"\n",
    "        }\n",
    "\n",
    "        if i == 0 or i == 1:\n",
    "            gender_or_birth_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params_2)\n",
    "            gender_or_birth_data = gender_or_birth_response.json()\n",
    "            \n",
    "            try:\n",
    "                entity_value = gender_or_birth_data[\"entities\"][entity_info_request_IDs[i]][\"labels\"][\"en\"][\"value\"]\n",
    "            except KeyError:\n",
    "                entity_info.append(None)\n",
    "            else:\n",
    "                entity_info.append(entity_value)\n",
    "\n",
    "        else:\n",
    "            employee_or_education_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params_2)\n",
    "            employee_or_education_data = employee_or_education_response.json()\n",
    "\n",
    "            try:\n",
    "                entity_IDs = entity_info_request_IDs[i].split(\"|\")\n",
    "            except AttributeError:\n",
    "                entity_IDs = []\n",
    "\n",
    "            for entity_ID in entity_IDs:\n",
    "                try:\n",
    "                    entity_value = employee_or_education_data[\"entities\"][entity_ID][\"labels\"][\"en\"][\"value\"]\n",
    "                except KeyError:\n",
    "                    entity_values.append(None)\n",
    "                else:\n",
    "                    entity_values.append(entity_value)\n",
    "            \n",
    "            entity_info.append(entity_values)\n",
    "    \n",
    "    return entity_info[0], entity_info[1], entity_info[2], entity_info[3], entity_info[4], entity_info[5], entity_info[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners = {\n",
    "    \"name\": [],\n",
    "    \"intro\": [],\n",
    "    \"birth_date\": [],\n",
    "    \"gender\": [],\n",
    "    \"birth_place\": [],\n",
    "    \"employer\": [],\n",
    "    \"educated_at\": []\n",
    "}\n",
    "\n",
    "for wikidata_ID in wikidata_IDs:\n",
    "    wikidata_name, wikipedia_intro, wikidata_birth_date, wikidata_gender, wikidata_birth_place, wikidata_employer, wikidata_educated_at = get_award_winners_info(wikidata_ID)\n",
    "    award_winners[\"name\"].append(wikidata_name)\n",
    "    award_winners[\"intro\"].append(wikipedia_intro)\n",
    "    award_winners[\"birth_date\"].append(wikidata_birth_date)\n",
    "    award_winners[\"gender\"].append(wikidata_gender)\n",
    "    award_winners[\"birth_place\"].append(wikidata_birth_place)\n",
    "    award_winners[\"employer\"].append(wikidata_employer)\n",
    "    award_winners[\"educated_at\"].append(wikidata_educated_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of all award winners are (alphabetical order): \n",
      "\n",
      "Adi Shamir, Alan Kay, Alan Perlis, Alfred Aho, Allen Newell, Amir Pnueli, Andrew Yao, Barbara Liskov, Bob Kahn, Butler Lampson, Charles Bachman, Charles P. Thacker, Dana Scott, David Patterson (computer scientist), Dennis Ritchie, Donald Knuth, Douglas Engelbart, E. Allen Emerson, Edgar F. Codd, Edmund M. Clarke, Edsger W. Dijkstra, Edward Feigenbaum, Edwin Catmull, Fernando J. Corbató, Frances Allen, Fred Brooks, Geoffrey Hinton, Herbert A. Simon, Ivan Sutherland, Jack Dongarra, James H. Wilkinson, Jeffrey Ullman, Jim Gray (computer scientist), John Backus, John Cocke (computer scientist), John Hopcroft, John L. Hennessy, John McCarthy (computer scientist), Joseph Sifakis, Judea Pearl, Juris Hartmanis, Ken Thompson, Kenneth E. Iverson, Kristen Nygaard, Leonard Adleman, Leslie Lamport, Leslie Valiant, Manuel Blum, Martin Hellman, Marvin Minsky, Maurice Wilkes, Michael O. Rabin, Michael Stonebraker, Niklaus Wirth, Ole-Johan Dahl, Pat Hanrahan, Peter Naur, Raj Reddy, Richard E. Stearns, Richard Hamming, Richard M. Karp, Robert Tarjan, Robert W. Floyd, Robin Milner, Ron Rivest, Shafi Goldwasser, Silvio Micali, Stephen Cook, Tim Berners-Lee, Tony Hoare, Vint Cerf, Whitfield Diffie, William Kahan, Yann LeCun, Yoshua Bengio.\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of all award winners are (alphabetical order): \\n\\n{}.\".format(\", \".join(sorted(award_winners[\"name\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners_intro = pd.DataFrame(columns = [\"winner_name\", \"count_words\", \"count_sentences\", \"count_paragraphs\", \"common_words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>359</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>the, of, and, He, a, is, Web, as, World, Wide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>and, the, of, for, Bengio, is, a, work, deep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geoffrey Hinton</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, and, of, for, in, Hinton, a, his, to, is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Knuth</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, of, and, Knuth, computer, is, to, He, ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard M. Karp</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>in, and, the, of, for, Karp, is, computer, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Fernando J. Corbató</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a, Fernando, José, \"Corby\", Corbató, (July, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Charles Bachman</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>his, Bachman, was, an, in, of, Charles, Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Butler Lampson</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Butler, W., Lampson,, ForMemRS,, (born, Decemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ole-Johan Dahl</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>of, Dahl, was, a, computer, the, and, Ole-Joha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Charles P. Thacker</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>computer, the, Charles, Patrick, \"Chuck\", Thac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            winner_name  count_words  count_sentences  count_paragraphs  \\\n",
       "0       Tim Berners-Lee          359               17                 4   \n",
       "1         Yoshua Bengio           91                4                 2   \n",
       "2       Geoffrey Hinton          181                8                 3   \n",
       "3          Donald Knuth          184                8                 3   \n",
       "4       Richard M. Karp           92                3                 2   \n",
       "..                  ...          ...              ...               ...   \n",
       "70  Fernando J. Corbató           28                1                 1   \n",
       "71      Charles Bachman           57                3                 1   \n",
       "72       Butler Lampson           27                1                 1   \n",
       "73       Ole-Johan Dahl           44                2                 1   \n",
       "74   Charles P. Thacker           35                2                 1   \n",
       "\n",
       "                                         common_words  \n",
       "0       the, of, and, He, a, is, Web, as, World, Wide  \n",
       "1   and, the, of, for, Bengio, is, a, work, deep, ...  \n",
       "2       the, and, of, for, in, Hinton, a, his, to, is  \n",
       "3   the, of, and, Knuth, computer, is, to, He, ana...  \n",
       "4   in, and, the, of, for, Karp, is, computer, the...  \n",
       "..                                                ...  \n",
       "70  a, Fernando, José, \"Corby\", Corbató, (July, 1,...  \n",
       "71  his, Bachman, was, an, in, of, Charles, Willia...  \n",
       "72  Butler, W., Lampson,, ForMemRS,, (born, Decemb...  \n",
       "73  of, Dahl, was, a, computer, the, and, Ole-Joha...  \n",
       "74  computer, the, Charles, Patrick, \"Chuck\", Thac...  \n",
       "\n",
       "[75 rows x 5 columns]"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "award_winners_intro[\"winner_name\"] = award_winners[\"name\"]\n",
    "award_winners_intro[\"count_words\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: x[0].split(), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"count_sentences\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: sent_tokenize(x[0]), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"count_paragraphs\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: x[0].split(\"\\n\"), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"common_words\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: FreqDist(x[0].split()).most_common(10), axis = 1).apply(lambda x: [i[0] for i in x]).apply(lambda x: \", \".join(x))\n",
    "award_winners_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words(\"english\")\n",
    "award_winners_intro_remove_stopwords = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: \" \".join([word for word in word_tokenize(x[0]) if word not in (en_stopwords)]), axis = 1)\n",
    "award_winners_intro_remove_stopwords_and_punctuation = award_winners_intro_remove_stopwords.apply(lambda x: RegexpTokenizer(r'\\w+').tokenize(x)).apply(lambda x: \" \".join(x))\n",
    "award_winners_intro[\"common_words_after_preprocessing\"] = award_winners_intro_remove_stopwords_and_punctuation.apply(lambda x: FreqDist(x.split()).most_common(10)).apply(lambda x: [i[0] for i in x]).apply(lambda x: \", \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "      <th>common_words_after_preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>359</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>the, of, and, He, a, is, Web, as, World, Wide</td>\n",
       "      <td>Web, He, World, Wide, Berners, Lee, s, Compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>and, the, of, for, Bengio, is, a, work, deep, ...</td>\n",
       "      <td>Bengio, work, deep, learning, Learning, Hinton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geoffrey Hinton</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, and, of, for, in, Hinton, a, his, to, is</td>\n",
       "      <td>Hinton, computer, work, neural, networks, Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Knuth</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, of, and, Knuth, computer, is, to, He, ana...</td>\n",
       "      <td>Knuth, computer, He, science, analysis, algori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard M. Karp</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>in, and, the, of, for, Karp, is, computer, the...</td>\n",
       "      <td>Karp, computer, theory, algorithms, Richard, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Fernando J. Corbató</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a, Fernando, José, \"Corby\", Corbató, (July, 1,...</td>\n",
       "      <td>July, Fernando, José, Corby, Corbató, 1, 1926,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Charles Bachman</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>his, Bachman, was, an, in, of, Charles, Willia...</td>\n",
       "      <td>Bachman, Charles, William, III, December, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Butler Lampson</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Butler, W., Lampson,, ForMemRS,, (born, Decemb...</td>\n",
       "      <td>Butler, W, Lampson, ForMemRS, born, December, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ole-Johan Dahl</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>of, Dahl, was, a, computer, the, and, Ole-Joha...</td>\n",
       "      <td>Dahl, computer, Ole, Johan, 12, October, 1931,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Charles P. Thacker</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>computer, the, Charles, Patrick, \"Chuck\", Thac...</td>\n",
       "      <td>computer, Charles, Patrick, Chuck, Thacker, Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            winner_name  count_words  count_sentences  count_paragraphs  \\\n",
       "0       Tim Berners-Lee          359               17                 4   \n",
       "1         Yoshua Bengio           91                4                 2   \n",
       "2       Geoffrey Hinton          181                8                 3   \n",
       "3          Donald Knuth          184                8                 3   \n",
       "4       Richard M. Karp           92                3                 2   \n",
       "..                  ...          ...              ...               ...   \n",
       "70  Fernando J. Corbató           28                1                 1   \n",
       "71      Charles Bachman           57                3                 1   \n",
       "72       Butler Lampson           27                1                 1   \n",
       "73       Ole-Johan Dahl           44                2                 1   \n",
       "74   Charles P. Thacker           35                2                 1   \n",
       "\n",
       "                                         common_words  \\\n",
       "0       the, of, and, He, a, is, Web, as, World, Wide   \n",
       "1   and, the, of, for, Bengio, is, a, work, deep, ...   \n",
       "2       the, and, of, for, in, Hinton, a, his, to, is   \n",
       "3   the, of, and, Knuth, computer, is, to, He, ana...   \n",
       "4   in, and, the, of, for, Karp, is, computer, the...   \n",
       "..                                                ...   \n",
       "70  a, Fernando, José, \"Corby\", Corbató, (July, 1,...   \n",
       "71  his, Bachman, was, an, in, of, Charles, Willia...   \n",
       "72  Butler, W., Lampson,, ForMemRS,, (born, Decemb...   \n",
       "73  of, Dahl, was, a, computer, the, and, Ole-Joha...   \n",
       "74  computer, the, Charles, Patrick, \"Chuck\", Thac...   \n",
       "\n",
       "                     common_words_after_preprocessing  \n",
       "0   Web, He, World, Wide, Berners, Lee, s, Compute...  \n",
       "1   Bengio, work, deep, learning, Learning, Hinton...  \n",
       "2   Hinton, computer, work, neural, networks, Goog...  \n",
       "3   Knuth, computer, He, science, analysis, algori...  \n",
       "4   Karp, computer, theory, algorithms, Richard, M...  \n",
       "..                                                ...  \n",
       "70  July, Fernando, José, Corby, Corbató, 1, 1926,...  \n",
       "71  Bachman, Charles, William, III, December, 11, ...  \n",
       "72  Butler, W, Lampson, ForMemRS, born, December, ...  \n",
       "73  Dahl, computer, Ole, Johan, 12, October, 1931,...  \n",
       "74  computer, Charles, Patrick, Chuck, Thacker, Fe...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "award_winners_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sub-activity: Applying NLP operations on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter = PorterStemmer()\n",
    "# stemmed = [porter.stem(word) for word in filtered_words]\n",
    "# print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Finding synonyms and antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sub-section: Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
