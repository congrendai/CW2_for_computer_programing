{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sub-activity: Loading and pre-processing of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "        \"format\":\"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "    \n",
    "    wikidata_ID_params = {\n",
    "    \"action\":\"query\",\n",
    "    \"format\":\"json\",\n",
    "    \"list\":\"search\",\n",
    "    \"srprop\":\"sectiontitle\",\n",
    "    \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"srlimit\":100\n",
    "    }\n",
    "\n",
    "    wikidata_ID_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_ID_params)\n",
    "    wikidata_ID_data = wikidata_ID_response.json()    \n",
    "    wikidata_IDs = [entity_ID[\"title\"] for entity_ID in wikidata_ID_data[\"query\"][\"search\"]]\n",
    "\n",
    "    return wikidata_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_IDs = get_turing_award_recipients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q80', 'Q3572699', 'Q92894', 'Q17457', 'Q92612', 'Q92638', 'Q92743', 'Q92824', 'Q181529', 'Q204815', 'Q578036', 'Q92794', 'Q92739', 'Q49823', 'Q92602', 'Q3571662', 'Q92626', 'Q92758', 'Q16080922', 'Q62870', 'Q8556', 'Q92604', 'Q357965', 'Q11609', 'Q92609', 'Q439245', 'Q92670', 'Q92819', 'Q92851', 'Q92613', 'Q62874', 'Q92854', 'Q92628', 'Q7143512', 'Q62861', 'Q320624', 'Q45575', 'Q1107006', 'Q92614', 'Q62888', 'Q93080', 'Q476466', 'Q92820', 'Q92649', 'Q62898', 'Q92641', 'Q92742', 'Q93154', 'Q62843', 'Q92643', 'Q92823', 'Q462089', 'Q62866', 'Q92629', 'Q92618', 'Q92822', 'Q92596', 'Q92746', 'Q918650', 'Q62857', 'Q92619', 'Q92821', 'Q62877', 'Q92782', 'Q92632', 'Q93161', 'Q92744', 'Q92606', 'Q92781', 'Q9602', 'Q92625', 'Q62894', 'Q92644', 'Q92745', 'Q92828']\n"
     ]
    }
   ],
   "source": [
    "print(wikidata_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(wikidata_ID):\n",
    "    wikipedia_API_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    WIKIDATA_GET_CONTENT_PARAMS = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "    \n",
    "    wikidata_response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_GET_CONTENT_PARAMS)\n",
    "    wikidata_response_data = wikidata_response.json()\n",
    "\n",
    "    # To extract content from the wikipidia page, we have to use titles gained the wikidata IDs, since the titles of wikipedia pages are unique.\n",
    "    wikidata_title = wikidata_response_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "\n",
    "    wikipedia_content_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": wikidata_title,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"explaintext\": 1,\n",
    "        \"exsectionformat\": \"wiki\",\n",
    "    }\n",
    "\n",
    "    wikipedia_content_response = requests.get(wikipedia_API_endpoint, params = wikipedia_content_params)\n",
    "    wikipedia_content_data = wikipedia_content_response.json()\n",
    "    wikipedia_content = wikipedia_content_data[\"query\"][\"pages\"][0][\"extract\"]\n",
    "    return wikipedia_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get__info(wikidata_ID):\n",
    "    \n",
    "    wikidata_title_params = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"props\": \"labels|claims\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"languages\": \"en\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "\n",
    "    wikidata_title_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_title_params)\n",
    "    wikidata_title_data = wikidata_title_response.json()\n",
    "\n",
    "    # Extract name\n",
    "    try:\n",
    "        wikidata_name = wikidata_title_data[\"entities\"][wikidata_ID][\"labels\"][\"en\"][\"value\"]\n",
    "        wikidata_names.append(wikidata_name)\n",
    "    except:\n",
    "        wikidata_name = None\n",
    "\n",
    "    # Extract intro from wikipedia page\n",
    "    try:\n",
    "        wikipedia_content = get_wikipedia_content(wikidata_ID)\n",
    "        wikipedia_intro = wikipedia_content.split(\"\\n\")[0]\n",
    "        wikipedia_intros.append(wikipedia_intro)\n",
    "    except:\n",
    "        wikipedia_intro = None\n",
    "    \n",
    "    # Extract gender ID to get gender from \"sex or gender (P21)\"\n",
    "    try:\n",
    "        wikidata_gender_id = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P21\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except:\n",
    "        wikidata_gender_id = None\n",
    "        \n",
    "    # Get birth date from \"date of birth (P569)\"\n",
    "    try:\n",
    "        wikidata_birth_date = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P569\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"].split(\"T\")[0].split(\"+\")[1]\n",
    "        wikidata_birth_dates.append(wikidata_birth_date)\n",
    "    except:\n",
    "        wikidata_birth_date = None\n",
    "\n",
    "    # Extract birth place ID to get birth place from \"place of birth (P19)\"\n",
    "    try:\n",
    "        wikidata_birth_place = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P19\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except:\n",
    "        wikidata_birth_place = None\n",
    "\n",
    "    # Extract employer ID to get employer from \"employer (P108)\"\n",
    "    # employer ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_employer_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P108\"]\n",
    "    except:\n",
    "        wikidata_employers_IDs = None\n",
    "    else:\n",
    "        wikidata_employers_IDs = [wikidata_employer_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_employer_ID in wikidata_employer_mainsnaks]\n",
    "\n",
    "    # Extract educated at ID to get educated at from \"educated at (P69)\"\n",
    "    # educated at ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_educated_at_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P69\"]\n",
    "    except:\n",
    "        wikidata_educated_at_IDs = None\n",
    "    else:\n",
    "        wikidata_educated_at_IDs = [wikidata_educated_at_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_educated_at_ID in wikidata_educated_at_mainsnaks]\n",
    "\n",
    "    entity_info_IDs = [wikidata_gender_id, wikidata_birth_place, \"|\".join(wikidata_employers_IDs), \"|\".join(wikidata_educated_at_IDs)]\n",
    "    print(entity_info_IDs)\n",
    "            \n",
    "    # for entity_info_ID in entity_info_IDs:\n",
    "    #     wikidata_title_params_2 = {\n",
    "    #         \"action\":\"wbgetentities\",\n",
    "    #         \"format\":\"json\",\n",
    "    #         \"ids\": entity_info_ID,\n",
    "    #         \"props\": \"labels\",\n",
    "    #         \"formatversion\": \"2\",\n",
    "    #         \"languages\": \"en\",\n",
    "    #         \"sitefilter\": \"enwiki\"\n",
    "    #     }\n",
    "\n",
    "    #     entity_info_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_title_params_2)\n",
    "    #     entity_info_data = entity_info_response.json()\n",
    "    #     entity_IDs = entity_info_ID.split(\"|\")\n",
    "    #     for entity_ID in entity_IDs:\n",
    "    #         entity_info = entity_info_data[\"entities\"][entity_ID][\"labels\"][\"en\"][\"value\"]\n",
    "    #         print(entity_info)\n",
    "\n",
    "    return wikidata_name, wikidata_birth_date, wikidata_birth_place, wikidata_employer, wikidata_educated_at\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "Pasadena\n",
      "PARC\n",
      "Digital Equipment Corporation\n",
      "University of California, Berkeley\n",
      "{'name': [], 'birth_date': [], 'birth_place': [], 'employer': [], 'educated_at': []}\n"
     ]
    }
   ],
   "source": [
    "award_winners = {\n",
    "    \"name\": [],\n",
    "    \"intro\": [],\n",
    "    \"gender\": [],\n",
    "    \"birth_date\": [],\n",
    "    \"birth_place\": [],\n",
    "    \"employer\": [],\n",
    "    \"educated_at\": []\n",
    "}\n",
    "\n",
    "for wikidata_ID in wikidata_IDs:\n",
    "    wikidata_name, wikipedia_intro, wikidata_gender, wikidata_birth_date, wikidata_birth_place, wikidata_employer, wikidata_educated_at = get__info(wikidata_ID)\n",
    "    award_winners[\"name\"].append(wikidata_name)\n",
    "    award_winners[\"intro\"].append(wikipedia_intro)\n",
    "    award_winners[\"gender\"].append(wikidata_gender)\n",
    "    award_winners[\"birth_date\"].append(wikidata_birth_date)\n",
    "    award_winners[\"birth_place\"].append(wikidata_birth_place)\n",
    "    award_winners[\"employer\"].append(wikidata_employer)\n",
    "    award_winners[\"educated_at\"].append(wikidata_educated_at)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of all award winners are (alphabetical order): \n",
      "\n",
      "Adi Shamir, Alan Kay, Alan Perlis, Alfred Aho, Allen Newell, Amir Pnueli, Andrew Yao, Barbara Liskov, Bob Kahn, Butler Lampson, Charles Bachman, Charles P. Thacker, Dana Scott, David A. Patterson, Dennis M. Ritchie, Donald Knuth, Douglas Engelbart, E. Allen Emerson, Edgar F. Codd, Edmund M. Clarke, Edsger W. Dijkstra, Edward Feigenbaum, Edwin Catmull, Fernando J. Corbató, Frances E. Allen, Fred Brooks, Geoffrey Hinton, Herbert Simon, Iosif Sifakis, Ivan Sutherland, Jack Dongarra, James H. Wilkinson, Jeffrey David Ullman, Jim Gray, John Backus, John Cocke, John Edward Hopcroft, John L. Hennessy, John McCarthy, Judea Pearl, Juris Hartmanis, Ken Thompson, Kenneth E. Iverson, Kristen Nygaard, Leonard Adleman, Leslie Lamport, Leslie Valiant, Manuel Blum, Martin Edward Hellman, Marvin Minsky, Maurice Wilkes, Michael O. Rabin, Michael Stonebraker, Niklaus Wirth, Ole-Johan Dahl, Pat Hanrahan, Peter Naur, Raj Reddy, Richard E. Stearns, Richard Hamming, Richard M. Karp, Robert Tarjan, Robert W. Floyd, Robin Milner, Ron Rivest, Shafrira Goldwasser, Silvio Micali, Stephen Cook, Tim Berners-Lee, Tony Hoare, Vint Cerf, Whitfield Diffie, William Kahan, Yann LeCun, Yoshua Bengio.\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of all award winners are (alphabetical order): \\n\\n{}.\".format(\", \".join(sorted([\"name\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name  birth_date  winner_name  count_words  \\\n",
      "0       Tim Berners-Lee  1955-06-08          NaN          NaN   \n",
      "1         Yoshua Bengio  1964-03-05          NaN          NaN   \n",
      "2       Geoffrey Hinton  1947-12-06          NaN          NaN   \n",
      "3          Donald Knuth  1938-01-10          NaN          NaN   \n",
      "4       Richard M. Karp  1935-01-03          NaN          NaN   \n",
      "..                  ...         ...          ...          ...   \n",
      "70  Fernando J. Corbató  1926-07-01          NaN          NaN   \n",
      "71      Charles Bachman  1924-12-11          NaN          NaN   \n",
      "72       Butler Lampson  1943-12-23          NaN          NaN   \n",
      "73       Ole-Johan Dahl  1931-10-12          NaN          NaN   \n",
      "74   Charles P. Thacker  1943-02-26          NaN          NaN   \n",
      "\n",
      "    count_sentences  count_paragraphs  common_words  \n",
      "0               NaN               NaN           NaN  \n",
      "1               NaN               NaN           NaN  \n",
      "2               NaN               NaN           NaN  \n",
      "3               NaN               NaN           NaN  \n",
      "4               NaN               NaN           NaN  \n",
      "..              ...               ...           ...  \n",
      "70              NaN               NaN           NaN  \n",
      "71              NaN               NaN           NaN  \n",
      "72              NaN               NaN           NaN  \n",
      "73              NaN               NaN           NaN  \n",
      "74              NaN               NaN           NaN  \n",
      "\n",
      "[75 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "_intro = pd.DataFrame()\n",
    "_intro[\"winner_name\"] = np.nan\n",
    "_intro[\"count_words\"] = np.nan\n",
    "_intro[\"count_sentences\"] = np.nan\n",
    "_intro[\"count_paragraphs\"] = np.nan\n",
    "_intro[\"common_words\"] = np.nan\n",
    "\n",
    "\n",
    "print(_intro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sub-activity: Applying NLP operations on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Finding synonyms and antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sub-section: Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
