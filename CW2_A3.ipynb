{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sub-activity: Loading and pre-processing of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "        \"format\":\"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "    \n",
    "    wikidata_ID_params = {\n",
    "    \"action\":\"query\",\n",
    "    \"format\":\"json\",\n",
    "    \"list\":\"search\",\n",
    "    \"srprop\":\"sectiontitle\",\n",
    "    \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"srlimit\":100\n",
    "    }\n",
    "\n",
    "    wikidata_ID_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_ID_params)\n",
    "    wikidata_ID_data = wikidata_ID_response.json()    \n",
    "    wikidata_IDs = [entity_ID[\"title\"] for entity_ID in wikidata_ID_data[\"query\"][\"search\"]]\n",
    "\n",
    "    return wikidata_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_IDs = get_turing_award_recipients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q80', 'Q3572699', 'Q92894', 'Q17457', 'Q92612', 'Q92638', 'Q92743', 'Q92824', 'Q181529', 'Q204815', 'Q578036', 'Q92794', 'Q92739', 'Q49823', 'Q92602', 'Q3571662', 'Q92626', 'Q92758', 'Q16080922', 'Q62870', 'Q8556', 'Q92604', 'Q357965', 'Q11609', 'Q92609', 'Q439245', 'Q92670', 'Q92819', 'Q92851', 'Q92613', 'Q62874', 'Q92854', 'Q92628', 'Q7143512', 'Q62861', 'Q320624', 'Q45575', 'Q1107006', 'Q92614', 'Q62888', 'Q93080', 'Q476466', 'Q92820', 'Q92649', 'Q62898', 'Q92641', 'Q92742', 'Q93154', 'Q62843', 'Q92643', 'Q92823', 'Q462089', 'Q62866', 'Q92629', 'Q92618', 'Q92822', 'Q92596', 'Q92746', 'Q918650', 'Q62857', 'Q92619', 'Q92821', 'Q62877', 'Q92782', 'Q92632', 'Q93161', 'Q92744', 'Q92606', 'Q92781', 'Q9602', 'Q92625', 'Q62894', 'Q92644', 'Q92745', 'Q92828']\n"
     ]
    }
   ],
   "source": [
    "print(wikidata_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(wikidata_ID):\n",
    "    wikipedia_API_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    WIKIDATA_GET_CONTENT_PARAMS = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "    \n",
    "    wikidata_response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_GET_CONTENT_PARAMS)\n",
    "    wikidata_response_data = wikidata_response.json()\n",
    "\n",
    "    # To extract content from the wikipidia page, we have to use titles gained the wikidata IDs, since the titles of wikipedia pages are unique.\n",
    "    wikidata_title = wikidata_response_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "\n",
    "    wikipedia_content_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": wikidata_title,\n",
    "        \"formatversion\": \"2\",\n",
    "        \"exsectionformat\": \"wiki\",\n",
    "    }\n",
    "\n",
    "    wikipedia_content_response = requests.get(wikipedia_API_endpoint, params = wikipedia_content_params)\n",
    "    wikipedia_content_data = wikipedia_content_response.json()\n",
    "    wikipedia_content = wikipedia_content_data[\"query\"][\"pages\"][0][\"extract\"]\n",
    "    return wikipedia_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q92743 = get_wikipedia_content(\"Q92743\")\n",
    "# Q80 = get_wikipedia_content(\"Q80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_paragraphs(content):\n",
    "#     cleaned_content = re.sub(r\"\\\\|\\n\",\"\", content)\n",
    "#     content_with_p_tag = re.sub(r\"<\\/?(?!p)\\w*\\b[^>]*>\", \"\", cleaned_content.split(\"<h2>\")[0])\n",
    "#     paragraphs = re.findall(r'<p>(.+?)</p>', content_with_p_tag)\n",
    "#     paragraphs = \" \\n\".join(paragraphs)\n",
    "#     return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for wikidata_ID in wikidata_IDs:\n",
    "#     content = get_wikipedia_content(wikidata_ID)\n",
    "#     paragraphs = get_paragraphs(content)\n",
    "#     print(paragraphs)\n",
    "#     print(\"lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_winners_info(wikidata_ID):\n",
    "    \n",
    "    wikidata_params = {\n",
    "        \"action\":\"wbgetentities\",\n",
    "        \"format\":\"json\",\n",
    "        \"ids\": wikidata_ID,\n",
    "        \"props\": \"claims|sitelinks\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"languages\": \"en\",\n",
    "        \"sitefilter\": \"enwiki\"\n",
    "    }\n",
    "\n",
    "    wikidata_title_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params)\n",
    "    wikidata_title_data = wikidata_title_response.json()\n",
    "\n",
    "    # Extract name\n",
    "    try:\n",
    "        wikidata_name = wikidata_title_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "    except KeyError:\n",
    "        wikidata_name = None\n",
    "\n",
    "    # Extract intro from wikipedia page\n",
    "    try:\n",
    "        wikipedia_content = get_wikipedia_content(wikidata_ID)\n",
    "        cleaned_content = re.sub(r\"\\\\|\\n\",\"\", wikipedia_content)\n",
    "        content_with_p_tag = re.sub(r\"<\\/?(?!p)\\w*\\b[^>]*>\", \"\", cleaned_content.split(\"<h2>\")[0])\n",
    "        paragraphs = re.findall(r'<p>(.+?)</p>', content_with_p_tag)\n",
    "        wikipedia_intro = \" \\n\".join(paragraphs)\n",
    "\n",
    "    except KeyError:\n",
    "        wikipedia_intro = None\n",
    "    \n",
    "    # Extract gender ID to get gender from \"sex or gender (P21)\"\n",
    "    try:\n",
    "        wikidata_gender_id = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P21\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_gender_id = None\n",
    "        \n",
    "    # Get birth date from \"date of birth (P569)\"\n",
    "    try:\n",
    "        wikidata_birth_date = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P569\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"].split(\"T\")[0].split(\"+\")[1]\n",
    "    except KeyError:\n",
    "        wikidata_birth_date = None\n",
    "\n",
    "    # Extract birth place ID to get birth place from \"place of birth (P19)\"\n",
    "    try:\n",
    "        wikidata_birth_place = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P19\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_birth_place = None\n",
    "\n",
    "    # Extract employer ID to get employer from \"employer (P108)\"\n",
    "    # employer ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_employer_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P108\"]\n",
    "    except KeyError:\n",
    "        wikidata_employers_IDs = None\n",
    "    else:\n",
    "        wikidata_employers_IDs = [wikidata_employer_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_employer_ID in wikidata_employer_mainsnaks]\n",
    "\n",
    "    # Extract educated at ID to get educated at from \"educated at (P69)\"\n",
    "    # educated at ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_educated_at_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P69\"]\n",
    "    except KeyError:\n",
    "        wikidata_educated_at_IDs = None\n",
    "    else:\n",
    "        wikidata_educated_at_IDs = [wikidata_educated_at_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_educated_at_ID in wikidata_educated_at_mainsnaks]\n",
    "\n",
    "    entity_info_request_IDs = [wikidata_gender_id, wikidata_birth_place, \"|\".join(wikidata_employers_IDs), \"|\".join(wikidata_educated_at_IDs)]\n",
    "    \n",
    "    entity_info = [wikidata_name, wikipedia_intro, wikidata_birth_date]\n",
    "    for i in range(len(entity_info_request_IDs)):\n",
    "        entity_values = []\n",
    "\n",
    "        wikidata_params_2 = {\n",
    "            \"action\":\"wbgetentities\",\n",
    "            \"format\":\"json\",\n",
    "            \"ids\": entity_info_request_IDs[i],\n",
    "            \"props\": \"labels\",\n",
    "            \"formatversion\": \"2\",\n",
    "            \"languages\": \"en\",\n",
    "            \"sitefilter\": \"enwiki\"\n",
    "        }\n",
    "\n",
    "        if i == 0 or i == 1:\n",
    "            gender_or_birth_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params_2)\n",
    "            gender_or_birth_data = gender_or_birth_response.json()\n",
    "            \n",
    "            try:\n",
    "                entity_value = gender_or_birth_data[\"entities\"][entity_info_request_IDs[i]][\"labels\"][\"en\"][\"value\"]\n",
    "                entity_info.append(entity_value)\n",
    "            except KeyError:\n",
    "                entity_info.append(None)\n",
    "\n",
    "        else:\n",
    "            employee_or_education_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params_2)\n",
    "            employee_or_education_data = employee_or_education_response.json()\n",
    "\n",
    "            try:\n",
    "                entity_IDs = entity_info_request_IDs[i].split(\"|\")\n",
    "            except AttributeError:\n",
    "                entity_IDs = []\n",
    "\n",
    "            for entity_ID in entity_IDs:\n",
    "                try:\n",
    "                    entity_value = employee_or_education_data[\"entities\"][entity_ID][\"labels\"][\"en\"][\"value\"]\n",
    "                    entity_values.append(entity_value)\n",
    "                except KeyError:\n",
    "                    entity_values.append(None)\n",
    "            \n",
    "            entity_info.append(entity_values)\n",
    "\n",
    "    # for entity_info_request_ID in entity_info_request_IDs:\n",
    "\n",
    "    #     wikidata_params_2 = {\n",
    "    #         \"action\":\"wbgetentities\",\n",
    "    #         \"format\":\"json\",\n",
    "    #         \"ids\": entity_info_request_ID,\n",
    "    #         \"props\": \"labels\",\n",
    "    #         \"formatversion\": \"2\",\n",
    "    #         \"languages\": \"en\",\n",
    "    #         \"sitefilter\": \"enwiki\"\n",
    "    #     }\n",
    "        \n",
    "    #     entity_values = []\n",
    "    #     entity_info_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_params_2)\n",
    "    #     entity_info_data = entity_info_response.json()\n",
    "\n",
    "    #     try:\n",
    "    #         entity_IDs = entity_info_request_ID.split(\"|\")\n",
    "    #     except AttributeError:\n",
    "    #         entity_IDs = []\n",
    "        \n",
    "    #     for entity_ID in entity_IDs:\n",
    "    #         try:\n",
    "    #             entity_value = entity_info_data[\"entities\"][entity_ID][\"labels\"][\"en\"][\"value\"]\n",
    "    #             entity_values.append(entity_value)\n",
    "    #         except KeyError:\n",
    "    #             entity_values.append(None)\n",
    "        \n",
    "    #     # print(entity_values)\n",
    "    #     entity_info.append(entity_values)\n",
    "    \n",
    "    return entity_info[0], entity_info[1], entity_info[2], entity_info[3], entity_info[4], entity_info[5], entity_info[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners = {\n",
    "    \"name\": [],\n",
    "    \"intro\": [],\n",
    "    \"birth_date\": [],\n",
    "    \"gender\": [],\n",
    "    \"birth_place\": [],\n",
    "    \"employer\": [],\n",
    "    \"educated_at\": []\n",
    "}\n",
    "\n",
    "for wikidata_ID in wikidata_IDs:\n",
    "    wikidata_name, wikipedia_intro, wikidata_birth_date, wikidata_gender, wikidata_birth_place, wikidata_employer, wikidata_educated_at = get_award_winners_info(wikidata_ID)\n",
    "    award_winners[\"name\"].append(wikidata_name)\n",
    "    award_winners[\"intro\"].append(wikipedia_intro)\n",
    "    award_winners[\"birth_date\"].append(wikidata_birth_date)\n",
    "    award_winners[\"gender\"].append(wikidata_gender)\n",
    "    award_winners[\"birth_place\"].append(wikidata_birth_place)\n",
    "    award_winners[\"employer\"].append(wikidata_employer)\n",
    "    award_winners[\"educated_at\"].append(wikidata_educated_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>intro</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>gender</th>\n",
       "      <th>birth_place</th>\n",
       "      <th>employer</th>\n",
       "      <th>educated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pat Hanrahan</td>\n",
       "      <td>Patrick M. Hanrahan (born 1954) is an American...</td>\n",
       "      <td>1954-00-00</td>\n",
       "      <td>male</td>\n",
       "      <td>None</td>\n",
       "      <td>[Stanford University]</td>\n",
       "      <td>[University of Wisconsin–Madison]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                              intro  \\\n",
       "33  Pat Hanrahan  Patrick M. Hanrahan (born 1954) is an American...   \n",
       "\n",
       "    birth_date gender birth_place               employer  \\\n",
       "33  1954-00-00   male        None  [Stanford University]   \n",
       "\n",
       "                          educated_at  \n",
       "33  [University of Wisconsin–Madison]  "
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(award_winners)\n",
    "a[a[\"birth_place\"].isnull() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of all award winners are (alphabetical order): \n",
      "\n",
      "Adi Shamir, Alan Kay, Alan Perlis, Alfred Aho, Allen Newell, Amir Pnueli, Andrew Yao, Barbara Liskov, Bob Kahn, Butler Lampson, Charles Bachman, Charles P. Thacker, Dana Scott, David Patterson (computer scientist), Dennis Ritchie, Donald Knuth, Douglas Engelbart, E. Allen Emerson, Edgar F. Codd, Edmund M. Clarke, Edsger W. Dijkstra, Edward Feigenbaum, Edwin Catmull, Fernando J. Corbató, Frances Allen, Fred Brooks, Geoffrey Hinton, Herbert A. Simon, Ivan Sutherland, Jack Dongarra, James H. Wilkinson, Jeffrey Ullman, Jim Gray (computer scientist), John Backus, John Cocke (computer scientist), John Hopcroft, John L. Hennessy, John McCarthy (computer scientist), Joseph Sifakis, Judea Pearl, Juris Hartmanis, Ken Thompson, Kenneth E. Iverson, Kristen Nygaard, Leonard Adleman, Leslie Lamport, Leslie Valiant, Manuel Blum, Martin Hellman, Marvin Minsky, Maurice Wilkes, Michael O. Rabin, Michael Stonebraker, Niklaus Wirth, Ole-Johan Dahl, Pat Hanrahan, Peter Naur, Raj Reddy, Richard E. Stearns, Richard Hamming, Richard M. Karp, Robert Tarjan, Robert W. Floyd, Robin Milner, Ron Rivest, Shafi Goldwasser, Silvio Micali, Stephen Cook, Tim Berners-Lee, Tony Hoare, Vint Cerf, Whitfield Diffie, William Kahan, Yann LeCun, Yoshua Bengio.\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of all award winners are (alphabetical order): \\n\\n{}.\".format(\", \".join(sorted(award_winners[\"name\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners_intro = pd.DataFrame(award_winners[\"intro\"], columns = [\"intro\"])\n",
    "award_winners_intro[\"winner_name\"] = np.nan\n",
    "award_winners_intro[\"count_words\"] = np.nan\n",
    "award_winners_intro[\"count_sentences\"] = np.nan\n",
    "award_winners_intro[\"count_paragraphs\"] = np.nan\n",
    "award_winners_intro[\"common_words\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners_intro[\"winner_name\"] = award_winners[\"name\"]\n",
    "award_winners_intro[\"count_words\"] = award_winners_intro[\"intro\"].apply(lambda x: len(word_tokenize(x)))\n",
    "award_winners_intro[\"count_sentences\"] = award_winners_intro[\"intro\"].apply(lambda x: len(sent_tokenize(x)))\n",
    "award_winners_intro[\"count_paragraphs\"] = award_winners_intro[\"intro\"].apply(lambda x: len(x.split(\"\\n\")))\n",
    "award_winners_intro[\"common_words\"] = award_winners_intro[\"intro\"].apply(lambda x: FreqDist(x.split()).most_common(10)).apply(lambda x: [i[0] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sir Timothy John Berners-Lee  (born 8 June 195...</td>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>411</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>[the, of, and, He, a, is, Web, as, World, Wide]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoshua Bengio  (born March 5, 1964) is a Canad...</td>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[and, the, of, for, Bengio, is, a, work, deep,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geoffrey Everest Hinton  (born 6 December 1947...</td>\n",
       "      <td>Geoffrey Hinton</td>\n",
       "      <td>205</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[the, and, of, for, in, Hinton, a, his, to, is]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Ervin Knuth ( kə-NOOTH; born January 10...</td>\n",
       "      <td>Donald Knuth</td>\n",
       "      <td>207</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>[the, of, and, Knuth, computer, is, to, He, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard Manning Karp (born January 3, 1935) is...</td>\n",
       "      <td>Richard M. Karp</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[in, and, the, of, for, Karp, is, computer, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Fernando José \"Corby\" Corbató (July 1, 1926 – ...</td>\n",
       "      <td>Fernando J. Corbató</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, Fernando, José, \"Corby\", Corbató, (July, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Charles William Bachman III (December 11, 1924...</td>\n",
       "      <td>Charles Bachman</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[his, Bachman, was, an, in, of, Charles, Willi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Butler W. Lampson, ForMemRS, (born December 23...</td>\n",
       "      <td>Butler Lampson</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[Butler, W., Lampson,, ForMemRS,, (born, Decem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ole-Johan Dahl (12 October 1931 – 29 June 2002...</td>\n",
       "      <td>Ole-Johan Dahl</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[of, Dahl, was, a, computer, the, and, Ole-Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Charles Patrick \"Chuck\" Thacker (February 26, ...</td>\n",
       "      <td>Charles P. Thacker</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[computer, the, Charles, Patrick, \"Chuck\", Tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                intro          winner_name  \\\n",
       "0   Sir Timothy John Berners-Lee  (born 8 June 195...      Tim Berners-Lee   \n",
       "1   Yoshua Bengio  (born March 5, 1964) is a Canad...        Yoshua Bengio   \n",
       "2   Geoffrey Everest Hinton  (born 6 December 1947...      Geoffrey Hinton   \n",
       "3   Donald Ervin Knuth ( kə-NOOTH; born January 10...         Donald Knuth   \n",
       "4   Richard Manning Karp (born January 3, 1935) is...      Richard M. Karp   \n",
       "..                                                ...                  ...   \n",
       "70  Fernando José \"Corby\" Corbató (July 1, 1926 – ...  Fernando J. Corbató   \n",
       "71  Charles William Bachman III (December 11, 1924...      Charles Bachman   \n",
       "72  Butler W. Lampson, ForMemRS, (born December 23...       Butler Lampson   \n",
       "73  Ole-Johan Dahl (12 October 1931 – 29 June 2002...       Ole-Johan Dahl   \n",
       "74  Charles Patrick \"Chuck\" Thacker (February 26, ...   Charles P. Thacker   \n",
       "\n",
       "    count_words  count_sentences  count_paragraphs  \\\n",
       "0           411               17                 4   \n",
       "1           110                4                 2   \n",
       "2           205                8                 3   \n",
       "3           207                8                 3   \n",
       "4           106                3                 2   \n",
       "..          ...              ...               ...   \n",
       "70           36                1                 1   \n",
       "71           65                2                 1   \n",
       "72           33                1                 1   \n",
       "73           48                2                 1   \n",
       "74           46                2                 1   \n",
       "\n",
       "                                         common_words  \n",
       "0     [the, of, and, He, a, is, Web, as, World, Wide]  \n",
       "1   [and, the, of, for, Bengio, is, a, work, deep,...  \n",
       "2     [the, and, of, for, in, Hinton, a, his, to, is]  \n",
       "3   [the, of, and, Knuth, computer, is, to, He, an...  \n",
       "4   [in, and, the, of, for, Karp, is, computer, th...  \n",
       "..                                                ...  \n",
       "70  [a, Fernando, José, \"Corby\", Corbató, (July, 1...  \n",
       "71  [his, Bachman, was, an, in, of, Charles, Willi...  \n",
       "72  [Butler, W., Lampson,, ForMemRS,, (born, Decem...  \n",
       "73  [of, Dahl, was, a, computer, the, and, Ole-Joh...  \n",
       "74  [computer, the, Charles, Patrick, \"Chuck\", Tha...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "award_winners_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intro</th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Joseph Sifakis (Greek: Ιωσήφ Σηφάκης) is a Gre...</td>\n",
       "      <td>Joseph Sifakis</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[Joseph, Sifakis, (Greek:, Ιωσήφ, Σηφάκης), is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                intro     winner_name  \\\n",
       "68  Joseph Sifakis (Greek: Ιωσήφ Σηφάκης) is a Gre...  Joseph Sifakis   \n",
       "\n",
       "    count_words  count_sentences  count_paragraphs  \\\n",
       "68           31                2                 1   \n",
       "\n",
       "                                         common_words  \n",
       "68  [Joseph, Sifakis, (Greek:, Ιωσήφ, Σηφάκης), is...  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "award_winners_intro[award_winners_intro[\"winner_name\"] == \"Joseph Sifakis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sub-activity: Applying NLP operations on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Finding synonyms and antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sub-section: Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
