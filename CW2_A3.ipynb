{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sub-activity: Loading and pre-processing of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "WIKIDATA_API_ENDPOINT = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "WIKIDATA_COMMON_PARAMS = {\n",
    "    \"ids\":\"\",\n",
    "    \"props\": \"\",\n",
    "    \"format\":\"json\",\n",
    "    \"languages\": \"en\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"sitefilter\": \"enwiki\",\n",
    "    \"action\": \"wbgetentities\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turing_award_recipients():\n",
    "\n",
    "    wikidata_entity_params = {\n",
    "        \"srlimit\":100,\n",
    "        \"format\":\"json\",\n",
    "        \"list\":\"search\",\n",
    "        \"action\":\"query\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"srprop\":\"sectiontitle\",\n",
    "        \"srsearch\": \"haswbstatement:P166=Q185667\",\n",
    "    }\n",
    "    \n",
    "    wikidata_ID_response = requests.get(WIKIDATA_API_ENDPOINT, params = wikidata_entity_params)\n",
    "    wikidata_ID_data = wikidata_ID_response.json()    \n",
    "    wikidata_IDs = [entity_ID[\"title\"] for entity_ID in wikidata_ID_data[\"query\"][\"search\"]]\n",
    "\n",
    "    return wikidata_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_IDs = get_turing_award_recipients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(wikidata_ID):\n",
    "    wikipedia_API_endpoint = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    WIKIDATA_COMMON_PARAMS[\"ids\"] = wikidata_ID\n",
    "    WIKIDATA_COMMON_PARAMS[\"props\"] = \"sitelinks\"\n",
    "    \n",
    "    wikidata_response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_COMMON_PARAMS)\n",
    "    wikidata_response_data = wikidata_response.json()\n",
    "\n",
    "    # To extract content from the wikipidia page, we have to use titles gained the wikidata IDs, since the titles of wikipedia pages are unique.\n",
    "    wikidata_title = wikidata_response_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]    \n",
    "\n",
    "    wikipedia_params = {\n",
    "        \"titles\": \"\",\n",
    "        \"format\": \"json\",\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"titles\": wikidata_title,\n",
    "        \"exsectionformat\": \"wiki\"\n",
    "    }\n",
    "    \n",
    "    wikipedia_content_response = requests.get(wikipedia_API_endpoint, params = wikipedia_params)\n",
    "    wikipedia_content_data = wikipedia_content_response.json()\n",
    "    wikipedia_content = wikipedia_content_data[\"query\"][\"pages\"][0][\"extract\"]\n",
    "    return wikipedia_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_award_winners_info(wikidata_ID):\n",
    "    \n",
    "    WIKIDATA_COMMON_PARAMS[\"ids\"] = wikidata_ID\n",
    "    WIKIDATA_COMMON_PARAMS[\"props\"] = \"claims|sitelinks\"\n",
    "\n",
    "    wikidata_title_response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_COMMON_PARAMS)\n",
    "    wikidata_title_data = wikidata_title_response.json()\n",
    "\n",
    "    # Extract name\n",
    "    try:\n",
    "        wikidata_name = wikidata_title_data[\"entities\"][wikidata_ID][\"sitelinks\"][\"enwiki\"][\"title\"]\n",
    "    except KeyError:\n",
    "        wikidata_name = None\n",
    "\n",
    "    # Extract intro from wikipedia page\n",
    "    try:\n",
    "        wikipedia_content = get_wikipedia_content(wikidata_ID)\n",
    "    except KeyError:\n",
    "        wikipedia_intro = None\n",
    "    else:\n",
    "        content_remove_newline_to_space = wikipedia_content.replace(\"\\n\", \" \")\n",
    "        content_with_p_tag = re.sub(r\"<\\/?(?!p)\\w*\\b[^>]*>\", \"\", content_remove_newline_to_space.split(\"<h2>\")[0])\n",
    "        paragraphs = re.findall(r'<p>(.+?)</p>', content_with_p_tag)\n",
    "        wikipedia_intro = \"\\n\".join(paragraphs)\n",
    "\n",
    "    # Extract gender ID to get gender from \"sex or gender (P21)\"\n",
    "    try:\n",
    "        wikidata_gender_id = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P21\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_gender_id = None\n",
    "        \n",
    "    # Get birth date from \"date of birth (P569)\"\n",
    "    try:\n",
    "        wikidata_birth_date = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P569\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"time\"].split(\"T\")[0].split(\"+\")[1]\n",
    "    except KeyError:\n",
    "        wikidata_birth_date = None\n",
    "\n",
    "    # Extract birth place ID to get birth place from \"place of birth (P19)\"\n",
    "    try:\n",
    "        wikidata_birth_place = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P19\"][0][\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"]\n",
    "    except KeyError:\n",
    "        wikidata_birth_place = None\n",
    "\n",
    "    # Extract employer ID to get employer from \"employer (P108)\"\n",
    "    # employer ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_employer_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P108\"]\n",
    "    except KeyError:\n",
    "        wikidata_employers_IDs = None\n",
    "    else:\n",
    "        wikidata_employers_IDs = [wikidata_employer_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_employer_ID in wikidata_employer_mainsnaks]\n",
    "        \n",
    "    # Extract educated at ID to get educated at from \"educated at (P69)\"\n",
    "    # educated at ID is inside of \"mainsnak\" key\n",
    "    try:\n",
    "        wikidata_educated_at_mainsnaks = wikidata_title_data[\"entities\"][wikidata_ID][\"claims\"][\"P69\"]\n",
    "    except KeyError:\n",
    "        wikidata_educated_at_IDs = None\n",
    "    else:\n",
    "        wikidata_educated_at_IDs = [wikidata_educated_at_ID[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for wikidata_educated_at_ID in wikidata_educated_at_mainsnaks]\n",
    "\n",
    "    # Join the IDs per person that we want to get info from into a list\n",
    "    entity_info_request_IDs = [wikidata_gender_id, wikidata_birth_place, \"|\".join(wikidata_employers_IDs), \"|\".join(wikidata_educated_at_IDs)]\n",
    "    \n",
    "    # A list that contains all the info of the person\n",
    "    \n",
    "    entity_info = [wikidata_name, wikipedia_intro, wikidata_birth_date]\n",
    "\n",
    "    WIKIDATA_COMMON_PARAMS[\"props\"] = \"labels\"\n",
    "    for i in range(len(entity_info_request_IDs)):\n",
    "        entity_values = []\n",
    "        \n",
    "        WIKIDATA_COMMON_PARAMS[\"ids\"] = entity_info_request_IDs[i]\n",
    "        \n",
    "        response = requests.get(WIKIDATA_API_ENDPOINT, params = WIKIDATA_COMMON_PARAMS)\n",
    "        data = response.json()\n",
    "\n",
    "        if i == 0 or i == 1:\n",
    "            try:\n",
    "                entity_value = data[\"entities\"][entity_info_request_IDs[i]][\"labels\"][\"en\"][\"value\"]\n",
    "            except KeyError:\n",
    "                entity_info.append(None)\n",
    "            else:\n",
    "                entity_info.append(entity_value)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                entity_IDs = entity_info_request_IDs[i].split(\"|\")\n",
    "            except AttributeError:\n",
    "                entity_IDs = []\n",
    "\n",
    "            for entity_ID in entity_IDs:\n",
    "                try:\n",
    "                    entity_value = data[\"entities\"][entity_ID][\"labels\"][\"en\"][\"value\"]\n",
    "                except KeyError:\n",
    "                    entity_values.append(None)\n",
    "                else:\n",
    "                    entity_values.append(entity_value)\n",
    "            \n",
    "            entity_info.append(entity_values)\n",
    "    \n",
    "    return entity_info[0], entity_info[1], entity_info[2], entity_info[3], entity_info[4], entity_info[5], entity_info[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners = {\n",
    "    \"name\": [],\n",
    "    \"intro\": [],\n",
    "    \"birth_date\": [],\n",
    "    \"gender\": [],\n",
    "    \"birth_place\": [],\n",
    "    \"employer\": [],\n",
    "    \"educated_at\": []\n",
    "}\n",
    "\n",
    "for wikidata_ID in wikidata_IDs:\n",
    "    wikidata_name, wikipedia_intro, wikidata_birth_date, wikidata_gender, wikidata_birth_place, wikidata_employer, wikidata_educated_at = get_award_winners_info(wikidata_ID)\n",
    "    award_winners[\"name\"].append(wikidata_name)\n",
    "    award_winners[\"intro\"].append(wikipedia_intro)\n",
    "    award_winners[\"birth_date\"].append(wikidata_birth_date)\n",
    "    award_winners[\"gender\"].append(wikidata_gender)\n",
    "    award_winners[\"birth_place\"].append(wikidata_birth_place)\n",
    "    award_winners[\"employer\"].append(wikidata_employer)\n",
    "    award_winners[\"educated_at\"].append(wikidata_educated_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Timothy John Berners-Lee  (born 8 June 1955), also known as TimBL, is an English computer scientist best known as the inventor of the World Wide Web. He is a Professorial Fellow of Computer Science at the University of Oxford and a professor at the Massachusetts Institute of Technology (MIT). Berners-Lee proposed an information management system on 12 March 1989, then implemented the first successful communication between a Hypertext Transfer Protocol (HTTP) client and server via the Internet in mid-November.\n",
      "Berners-Lee is the director of the World Wide Web Consortium (W3C), which oversees the continued development of the Web. He co-founded (with his then wife-to-be Rosemary Leith) the World Wide Web Foundation. He is a senior researcher and holder of the 3Com founder's chair at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). He is a director of the Web Science Research Initiative (WSRI) and a member of the advisory board of the MIT Center for Collective Intelligence. In 2011, he was named as a member of the board of trustees of the Ford Foundation. He is a founder and president of the Open Data Institute and is currently an advisor at social network MeWe.\n",
      "He devised and implemented the first Web browser and Web server, and helped foster the Web's subsequent explosive development. He currently directs the W3 Consortium, developing tools and standards to further the Web's potential. In April 2009, he was elected as Foreign Associate of the National Academy of Sciences.\n",
      "In 2004, Berners-Lee was knighted by Queen Elizabeth II for his pioneering work. He was named in Time magazine's list of the 100 Most Important People of the 20th century and has received a number of other accolades for his invention. He was honoured as the \"Inventor of the World Wide Web\" during the 2012 Summer Olympics opening ceremony in which he appeared working with a vintage NeXT Computer. He tweeted \"This is for everyone\" which appeared in LED lights attached to the chairs of the audience. He received the 2016 Turing Award \"for inventing the World Wide Web, the first web browser, and the fundamental protocols and algorithms allowing the Web to scale\".\n"
     ]
    }
   ],
   "source": [
    "print(award_winners[\"intro\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of all award winners are (alphabetical order): \n",
      "\n",
      "Adi Shamir, Alan Kay, Alan Perlis, Alfred Aho, Allen Newell, Amir Pnueli, Andrew Yao, Barbara Liskov, Bob Kahn, Butler Lampson, Charles Bachman, Charles P. Thacker, Dana Scott, David Patterson (computer scientist), Dennis Ritchie, Donald Knuth, Douglas Engelbart, E. Allen Emerson, Edgar F. Codd, Edmund M. Clarke, Edsger W. Dijkstra, Edward Feigenbaum, Edwin Catmull, Fernando J. Corbató, Frances Allen, Fred Brooks, Geoffrey Hinton, Herbert A. Simon, Ivan Sutherland, Jack Dongarra, James H. Wilkinson, Jeffrey Ullman, Jim Gray (computer scientist), John Backus, John Cocke (computer scientist), John Hopcroft, John L. Hennessy, John McCarthy (computer scientist), Joseph Sifakis, Judea Pearl, Juris Hartmanis, Ken Thompson, Kenneth E. Iverson, Kristen Nygaard, Leonard Adleman, Leslie Lamport, Leslie Valiant, Manuel Blum, Martin Hellman, Marvin Minsky, Maurice Wilkes, Michael O. Rabin, Michael Stonebraker, Niklaus Wirth, Ole-Johan Dahl, Pat Hanrahan, Peter Naur, Raj Reddy, Richard E. Stearns, Richard Hamming, Richard M. Karp, Robert Tarjan, Robert W. Floyd, Robin Milner, Ron Rivest, Shafi Goldwasser, Silvio Micali, Stephen Cook, Tim Berners-Lee, Tony Hoare, Vint Cerf, Whitfield Diffie, William Kahan, Yann LeCun, Yoshua Bengio.\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of all award winners are (alphabetical order): \\n\\n{}.\".format(\", \".join(sorted(award_winners[\"name\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_winners_intro = pd.DataFrame(columns = [\"winner_name\", \"count_words\", \"count_sentences\", \"count_paragraphs\", \"common_words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>359</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>the, of, and, He, a, is, Web, as, World, Wide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>and, the, of, for, Bengio, is, a, work, deep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geoffrey Hinton</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, and, of, for, in, Hinton, a, his, to, is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Knuth</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, of, and, Knuth, computer, is, to, He, ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard M. Karp</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>in, and, the, of, for, Karp, is, computer, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Fernando J. Corbató</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a, Fernando, José, \"Corby\", Corbató, (July, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Charles Bachman</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>his, Bachman, was, an, in, of, Charles, Willia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Butler Lampson</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Butler, W., Lampson,, ForMemRS,, (born, Decemb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ole-Johan Dahl</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>of, Dahl, was, a, computer, the, and, Ole-Joha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Charles P. Thacker</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>computer, the, Charles, Patrick, \"Chuck\", Thac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            winner_name  count_words  count_sentences  count_paragraphs  \\\n",
       "0       Tim Berners-Lee          359               17                 4   \n",
       "1         Yoshua Bengio           91                4                 2   \n",
       "2       Geoffrey Hinton          181                8                 3   \n",
       "3          Donald Knuth          184                8                 3   \n",
       "4       Richard M. Karp           92                3                 2   \n",
       "..                  ...          ...              ...               ...   \n",
       "70  Fernando J. Corbató           28                1                 1   \n",
       "71      Charles Bachman           57                3                 1   \n",
       "72       Butler Lampson           27                1                 1   \n",
       "73       Ole-Johan Dahl           44                2                 1   \n",
       "74   Charles P. Thacker           35                2                 1   \n",
       "\n",
       "                                         common_words  \n",
       "0       the, of, and, He, a, is, Web, as, World, Wide  \n",
       "1   and, the, of, for, Bengio, is, a, work, deep, ...  \n",
       "2       the, and, of, for, in, Hinton, a, his, to, is  \n",
       "3   the, of, and, Knuth, computer, is, to, He, ana...  \n",
       "4   in, and, the, of, for, Karp, is, computer, the...  \n",
       "..                                                ...  \n",
       "70  a, Fernando, José, \"Corby\", Corbató, (July, 1,...  \n",
       "71  his, Bachman, was, an, in, of, Charles, Willia...  \n",
       "72  Butler, W., Lampson,, ForMemRS,, (born, Decemb...  \n",
       "73  of, Dahl, was, a, computer, the, and, Ole-Joha...  \n",
       "74  computer, the, Charles, Patrick, \"Chuck\", Thac...  \n",
       "\n",
       "[75 rows x 5 columns]"
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "award_winners_intro[\"winner_name\"] = award_winners[\"name\"]\n",
    "award_winners_intro[\"count_words\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: x[0].split(), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"count_sentences\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: sent_tokenize(x[0]), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"count_paragraphs\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: x[0].split(\"\\n\"), axis = 1).apply(lambda x: len(x))\n",
    "award_winners_intro[\"common_words\"] = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: FreqDist(x[0].split()).most_common(10), axis = 1).apply(lambda x: [i[0] for i in x]).apply(lambda x: \", \".join(x))\n",
    "award_winners_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner_name</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_sentences</th>\n",
       "      <th>count_paragraphs</th>\n",
       "      <th>common_words</th>\n",
       "      <th>common_words_after_preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>359</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>the, of, and, He, a, is, Web, as, World, Wide</td>\n",
       "      <td>Web, He, World, Wide, Berners, Lee, s, Compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yoshua Bengio</td>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>and, the, of, for, Bengio, is, a, work, deep, ...</td>\n",
       "      <td>Bengio, work, deep, learning, Learning, Hinton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Geoffrey Hinton</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, and, of, for, in, Hinton, a, his, to, is</td>\n",
       "      <td>Hinton, computer, work, neural, networks, Goog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Donald Knuth</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>the, of, and, Knuth, computer, is, to, He, ana...</td>\n",
       "      <td>Knuth, computer, He, science, analysis, algori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard M. Karp</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>in, and, the, of, for, Karp, is, computer, the...</td>\n",
       "      <td>Karp, computer, theory, algorithms, Richard, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Fernando J. Corbató</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a, Fernando, José, \"Corby\", Corbató, (July, 1,...</td>\n",
       "      <td>July, Fernando, José, Corby, Corbató, 1, 1926,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Charles Bachman</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>his, Bachman, was, an, in, of, Charles, Willia...</td>\n",
       "      <td>Bachman, Charles, William, III, December, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Butler Lampson</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Butler, W., Lampson,, ForMemRS,, (born, Decemb...</td>\n",
       "      <td>Butler, W, Lampson, ForMemRS, born, December, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ole-Johan Dahl</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>of, Dahl, was, a, computer, the, and, Ole-Joha...</td>\n",
       "      <td>Dahl, computer, Ole, Johan, 12, October, 1931,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Charles P. Thacker</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>computer, the, Charles, Patrick, \"Chuck\", Thac...</td>\n",
       "      <td>computer, Charles, Patrick, Chuck, Thacker, Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            winner_name  count_words  count_sentences  count_paragraphs  \\\n",
       "0       Tim Berners-Lee          359               17                 4   \n",
       "1         Yoshua Bengio           91                4                 2   \n",
       "2       Geoffrey Hinton          181                8                 3   \n",
       "3          Donald Knuth          184                8                 3   \n",
       "4       Richard M. Karp           92                3                 2   \n",
       "..                  ...          ...              ...               ...   \n",
       "70  Fernando J. Corbató           28                1                 1   \n",
       "71      Charles Bachman           57                3                 1   \n",
       "72       Butler Lampson           27                1                 1   \n",
       "73       Ole-Johan Dahl           44                2                 1   \n",
       "74   Charles P. Thacker           35                2                 1   \n",
       "\n",
       "                                         common_words  \\\n",
       "0       the, of, and, He, a, is, Web, as, World, Wide   \n",
       "1   and, the, of, for, Bengio, is, a, work, deep, ...   \n",
       "2       the, and, of, for, in, Hinton, a, his, to, is   \n",
       "3   the, of, and, Knuth, computer, is, to, He, ana...   \n",
       "4   in, and, the, of, for, Karp, is, computer, the...   \n",
       "..                                                ...   \n",
       "70  a, Fernando, José, \"Corby\", Corbató, (July, 1,...   \n",
       "71  his, Bachman, was, an, in, of, Charles, Willia...   \n",
       "72  Butler, W., Lampson,, ForMemRS,, (born, Decemb...   \n",
       "73  of, Dahl, was, a, computer, the, and, Ole-Joha...   \n",
       "74  computer, the, Charles, Patrick, \"Chuck\", Thac...   \n",
       "\n",
       "                     common_words_after_preprocessing  \n",
       "0   Web, He, World, Wide, Berners, Lee, s, Compute...  \n",
       "1   Bengio, work, deep, learning, Learning, Hinton...  \n",
       "2   Hinton, computer, work, neural, networks, Goog...  \n",
       "3   Knuth, computer, He, science, analysis, algori...  \n",
       "4   Karp, computer, theory, algorithms, Richard, M...  \n",
       "..                                                ...  \n",
       "70  July, Fernando, José, Corby, Corbató, 1, 1926,...  \n",
       "71  Bachman, Charles, William, III, December, 11, ...  \n",
       "72  Butler, W, Lampson, ForMemRS, born, December, ...  \n",
       "73  Dahl, computer, Ole, Johan, 12, October, 1931,...  \n",
       "74  computer, Charles, Patrick, Chuck, Thacker, Fe...  \n",
       "\n",
       "[75 rows x 6 columns]"
      ]
     },
     "execution_count": 1142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords = stopwords.words(\"english\")\n",
    "award_winners_intro_remove_stopwords = pd.DataFrame(award_winners[\"intro\"]).apply(lambda x: \" \".join([word for word in word_tokenize(x[0]) if word not in (en_stopwords)]), axis = 1)\n",
    "award_winners_intro_remove_stopwords_and_punctuation = award_winners_intro_remove_stopwords.apply(lambda x: RegexpTokenizer(r'\\w+').tokenize(x)).apply(lambda x: \" \".join(x))\n",
    "award_winners_intro[\"common_words_after_preprocessing\"] = award_winners_intro_remove_stopwords_and_punctuation.apply(lambda x: FreqDist(x.split()).most_common(10)).apply(lambda x: [i[0] for i in x]).apply(lambda x: \", \".join(x))\n",
    "award_winners_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        winner_name  count_words  count_sentences  count_paragraphs  \\\n",
      "0   Tim Berners-Lee          359               17                 4   \n",
      "1     Yoshua Bengio           91                4                 2   \n",
      "2   Geoffrey Hinton          181                8                 3   \n",
      "3      Donald Knuth          184                8                 3   \n",
      "4   Richard M. Karp           92                3                 2   \n",
      "5     Robert Tarjan           62                3                 1   \n",
      "6         Vint Cerf           65                2                 1   \n",
      "7       Judea Pearl          156                5                 2   \n",
      "8  Herbert A. Simon          181                7                 2   \n",
      "9     Marvin Minsky           54                2                 2   \n",
      "\n",
      "                                        common_words  \\\n",
      "0      the, of, and, He, a, is, Web, as, World, Wide   \n",
      "1  and, the, of, for, Bengio, is, a, work, deep, ...   \n",
      "2      the, and, of, for, in, Hinton, a, his, to, is   \n",
      "3  the, of, and, Knuth, computer, is, to, He, ana...   \n",
      "4  in, and, the, of, for, Karp, is, computer, the...   \n",
      "5  and, is, the, of, Tarjan, at, Robert, Endre, (...   \n",
      "6  the, of, and, is, National, Medal, Vinton, Gra...   \n",
      "7     the, and, of, for, is, on, Pearl, a, in, Judea   \n",
      "8  the, of, and, was, in, science,, to, political...   \n",
      "9  and, of, Minsky, the, AI, Marvin, Lee, (August...   \n",
      "\n",
      "                    common_words_after_preprocessing  \n",
      "0  Web, He, World, Wide, Berners, Lee, s, Compute...  \n",
      "1  Bengio, work, deep, learning, Learning, Hinton...  \n",
      "2  Hinton, computer, work, neural, networks, Goog...  \n",
      "3  Knuth, computer, He, science, analysis, algori...  \n",
      "4  Karp, computer, theory, algorithms, Richard, M...  \n",
      "5  Tarjan, University, Robert, Endre, born, April...  \n",
      "6  Internet, National, Medal, Vinton, Gray, Cerf,...  \n",
      "7  Pearl, Judea, American, computer, probabilisti...  \n",
      "8  science, political, computer, He, Simon, 2001,...  \n",
      "9  AI, Minsky, Marvin, Lee, August, 9, 1927, Janu...  \n"
     ]
    }
   ],
   "source": [
    "print(award_winners_intro.iloc[0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sub-activity: Applying NLP operations on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/congrendai/Desktop/King's/Computer Programming for Data Scientists/CW2/CW2_A3.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/congrendai/Desktop/King%27s/Computer%20Programming%20for%20Data%20Scientists/CW2/CW2_A3.ipynb#Z1451sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(intro)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/congrendai/Desktop/King%27s/Computer%20Programming%20for%20Data%20Scientists/CW2/CW2_A3.ipynb#Z1451sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m en_stopwords]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/congrendai/Desktop/King%27s/Computer%20Programming%20for%20Data%20Scientists/CW2/CW2_A3.ipynb#Z1451sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m RegexpTokenizer(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mw+\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mtokenize(words)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/nltk/tokenize/regexp.py:133\u001b[0m, in \u001b[0;36mRegexpTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_regexp\u001b[39m.\u001b[39msplit(text)\n\u001b[1;32m    131\u001b[0m \u001b[39m# If our regexp matches tokens, use re.findall:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_regexp\u001b[39m.\u001b[39;49mfindall(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "for intro in award_winners[\"intro\"]:\n",
    "    tokens = word_tokenize(intro)\n",
    "    words = [word for word in tokens if word not in en_stopwords]\n",
    "    RegexpTokenizer(r'\\w+').tokenize(words)\n",
    "    \n",
    "# porter = PorterStemmer()\n",
    "# stemmed = [porter.stem(word) for word in filtered_words]\n",
    "# print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Finding synonyms and antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Bigrams and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sub-section: Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Barplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
